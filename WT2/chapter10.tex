\chapter*{10. Konvergenz von Zufallsvariablen}
\addcontentsline{toc}{chapter}{10. Konvergenz von Zufallsvariablen}

\section*{Gesetze der gro\ss{}en Zahlen}
\addcontentsline{toc}{section}{Gesetze der gro\ss{}en Zahlen}

Sei im folgenden Kapitel $(\Omega,\mathcal{A},\Pp)$ jeweils ein
Wahrscheinlichkeitsraum und $X_n,X\in L^1, n\geq1$ jeweils
$\overline{\R}$-wertige Zufallsvariablen.

\paragraph{10.1. Proposition:} Sei $X\in L^2$. Dann ist folgendes
\"aquivalent:
\begin{enumerate}
    \item $\dfrac{1}{n}\displaystyle\sum_{i=1}^n(X_i-\E X_i)\nto{L^2}{n\to\infty}0$
	\item $\dfrac{1}{n^2}\displaystyle\sum_{i=1}^n\sum_{j=1}^n\operatorname{Cov}(X_i,X_j)\nto{}{n\to\infty}0$
\end{enumerate}

\paragraph{Beweis:} 
\begin{equation*}
    \E\left[\left(\dfrac{1}{n}\sum_{i=1}^n(X_i-\E X_i\right)^2\right]=\dfrac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n\E\left[(X_i-\E X_i)(X_j-\E X_j)\right]
\end{equation*}


\paragraph{Bermerkung:} F\"ur $X_i\in L^2$ unkorreliert ist 2. \"aquivalent zu $$\dfrac{1}{n^2}\sum_{i=1}^n\Var(X_i)\nto{}{}0$$ Falls die $X_i\in L^2$ und i.i.d. sind, gilt 1. und 2. trivial.


\paragraph{10.2. Satz (Schwaches Gesetz der gro\ss{}en Zahlen, WLLN):} Seien $X_i\in L^1$ i.i.d.. Dann gilt: 
    $$\dfrac{1}{n}\sum_{i=1}^nX_i\nto{P}{n\to\infty}\E X_1$$ 

\paragraph{Beweis:} Sei $M>0$. Dann gilt
\begin{align*}
    \dfrac{1}{n}\sum_{i=1}^n(X_i-\E X_i)
    &=\dfrac{1}{n}\sum_{i=1}^n\left(X_i\cdot\ind{\{|X_i|\leq M\}}-\E [X_i\cdot\ind{\{|X_i|\leq M\}}]\right)\\
    &+\dfrac{1}{n}\sum_{i=1}^n\left(X_i\cdot\ind{\{|X_i|> M\}}-\E[ X_i\cdot\ind{\{|X_i|> M\}}]\right)
\end{align*}
\begin{enumerate}[label=\Roman*.]
    \item Zum ersten Summanden: \newline
    Die Zufallsvariablen $X_i\cdot\ind{\{|X_i|\leq M\}}$ sind unabh\"angig und beschr\"ankt (also $\in L^\infty$ und damit insbesondere $\in L^2$). Damit ist
    $$\Var\left(X_i\cdot\ind{\{|X_i|\leq M\}}\right)=\E\left[X_i^2\cdot\ind{\{|X_i|\leq M\}}\right]-\left(\E\left[X_i\cdot\ind{\{|X_i|\leq M\}}\right]\right)^2\leq 2M^2$$
    Mit Bemerkung (ii) zu Proposition 10.1 folgt damit, dass der erste Summand $\nto{L^2}{n\to\infty}$ und damit $\nto{P}{n\to\infty}0$.
    \item Zum zweiten Summanden:\newline
    \begin{align*}
        \E\left|\dfrac{1}{n}\sum_{i=1}^n\left(X_i\cdot\ind{\{|X_i|> M\}}-\E[ X_i\cdot\ind{\{|X_i|> M\}}]\right)\right|
        &\overset{\text{2x DUG}}{\leq}
        \E\left[\dfrac{1}{n}\sum_{i=1}^n\left|X_i\cdot\ind{\{|X_i|> M\}}\right|+\left|\E[ X_i\cdot\ind{\{|X_i|> M\}}]\right|\right]\\
        &\leq\dfrac{1}{n}\sum_{i=1}^n\E\left|X_i\cdot\ind{\{|X_i|>M\}}\right|+\E[ |X_i|\cdot\ind{\{|X_i|> M\}}]\\
        &=2\cdot\E\left[|X_1|\cdot\ind{\{|X_i|>M\}}\right]\nto{}{M\to\infty}0
    \end{align*}
\end{enumerate}
Sei also $\eps>0$. Dann ist
\begin{align*}
    \limsup_{n\to\infty}\Pp\left(\left|\dfrac{1}{n}\sum_{i=1}^n(X_i-\E X_i)\right|>\eps\right)&=\limsup_{n\to\infty}\Pp(|(1)+(2)|>\eps)\\
    &\leq\limsup_{n\to\infty}\Pp\left(|(1)|>\dfrac{\eps}{2}\right)+\Pp\left(|(2)|>\dfrac{\eps}{2}\right)\\
    &=\limsup_{n\to\infty}\Pp\left(|(2)|>\dfrac{\eps}{2}\right)\\
    &\overset{\text{Markov}}{\leq}\limsup_{n\to\infty}\dfrac{2\cdot\E|(2)|}{\eps}\\
    &\leq\dfrac{4\cdot\E\left[|X_1|\cdot\ind{\{|X_i|>M\}}\right]}{\eps}
\end{align*}
f\"ur jedes $M>0$. W\"ahle nun f\"ur $\eps>0$ ein $M>0$ so, sodass dieser Ausdruck $<\delta$ ist. Das funktioniert wegen II. f\"ur jedes $\delta>0$ und daher folgt 
$$\Pp\left(\left|\dfrac{1}{n}\sum_{i=1}^n(X_i-\E X_i)\right|>\eps\right)\nto{}{n\to\infty}0$$
und damit die Aussage. \qed

\paragraph{10.3. Satz (Starkes Gesetz der gro\ss{}en Zahlen, SLLN):} Seien
$X_i\in L^1$ i.i.d.. Dann gilt sogar die st\"arkere Aussage: 
$$\dfrac{1}{n}\sum_{i=1}^nX_i\nto{a.s.}{n\to\infty}\E X_1$$ 

\paragraph{Beweis:} siehe z.B.: P. Billingsley, \textit{Probability and Measure} (2nd Ed.), Theorem 6.1 \qed

\paragraph{10.4. Proposition (Momentenmethode):} Betrachte i.i.d. Zufallsvariablen $X_i,i\geq1$, sodass $X_i^d\in L^1$ f\"ur ein $d\geq1$. Sei außerdem $f:\R^d\to\R^\ell$ und setze $\theta:=f(\E X_1^1,\hdots,\E X_1^d)$ ($\theta$ könnte z.B. als Funktion der ersten $d$ Momente die Verteilung von $X_1$ parametrisieren). Falls $f$ stetig im Punkt $(\E X_1^1,\hdots,\E X_1^d)'$ ist, dann gilt f\"ur $\hat\theta_n:=f\left(\frac{1}{n}\sum_{i=1}^nX_i^1,\hdots,\frac{1}{n}\sum_{i=1}^nX_i^d\right)$, dass $\hat\theta_n\nto{P}{n\to\infty}\theta$.

\paragraph{Beweis:}Mit der Ljapunov-Unggleichung gilt $X_i^j\in L^1$ f\"ur $j=1,\hdots,d$. Weiters sind $X_i^j$ i.i.d. Mit dem SLLN gilt damit f\"ur $j=1,\hdots,d$
$$\dfrac{1}{n}\sum_{i=1}^nX_i^j\nto{a.s.}{n\to\infty}\E X_1^j$$
und mit Proposition 9.15 folgt 
$$\left(\frac{1}{n}\sum_{i=1}^nX_i^1,\hdots,\frac{1}{n}\sum_{i=1}^nX_i^d\right)'\nto{a.s.}{n\to\infty}(\E X_1^1,\hdots,\E X_1^d)'$$
Die Aussage folgt schließlich mit dem CMT (quasi Satz 9.13. f\"ur $f:\R^d\to\R^\ell$). \qed 
